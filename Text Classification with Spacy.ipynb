{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f72d7-8aef-48be-b230-94802d1a9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"numpy<2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90715f44-fcad-4706-b52d-e8a4c5ae5286",
   "metadata": {},
   "source": [
    "## Text Classification with SpaCy\n",
    " #### 1.Load SpaCy and process text.\n",
    " #### 2.Extract linguistic features (POS, entities, etc.).\n",
    " #### 3.Train a classifier using features.\n",
    " #### 4. Evaluate the classifier.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03de28b-4c76-4d64-a726-a106186d0ea4",
   "metadata": {},
   "source": [
    "#### Import Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f98dcc4-4c4a-4ea8-a972-abf4747ae266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d64e8-86e3-4c0b-a3d1-bf459d671f32",
   "metadata": {},
   "source": [
    "#### We create a small dataset with sentences labeled as \"pos\" (positive) or \"neg\" (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ee98ce-11fe-4a3a-a918-b852afc54c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this movie | Label: pos\n",
      "Text: This film is amazing | Label: pos\n",
      "Text: I hated this movie | Label: neg\n",
      "Text: This film is terrible | Label: neg\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample dataset of sentences labeled as positive or negative\n",
    "training_data = [\n",
    "    (\"I love this movie\", \"pos\"),\n",
    "    (\"This film is amazing\", \"pos\"),\n",
    "    (\"I hated this movie\", \"neg\"),\n",
    "    (\"This film is terrible\", \"neg\")\n",
    "]\n",
    "\n",
    "# Print the training data to verify\n",
    "for sentence, label in training_data:\n",
    "    print(f\"Text: {sentence} | Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f054f-43e6-433b-a2c3-c89f1a3e8fc3",
   "metadata": {},
   "source": [
    "##  Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d39d38-992f-416a-a6ec-5d30ee3f4d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'love': True, 'programming': True}, 'positive'), ({'boring': True}, 'negative'), ({'python': True, 'awesome': True}, 'positive'), ({'hate': True, 'bugs': True}, 'negative')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\neeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def extract_features(sentence):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return {word: True for word in words}\n",
    "\n",
    "# Define training data\n",
    "training_data = [\n",
    "    (\"I love programming\", \"positive\"),\n",
    "    (\"This is so boring\", \"negative\"),\n",
    "    (\"Python is awesome\", \"positive\"),\n",
    "    (\"I hate bugs\", \"negative\")\n",
    "]\n",
    "\n",
    "training_features = [(extract_features(sentence), label) for sentence, label in training_data]\n",
    "print(training_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff927d-5fcc-4252-8174-35ebe2117ba2",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier: We train the classifier on the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83420bba-1c93-4caf-98a3-0af98caffbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 3: Train Naive Bayes Classifier\n",
    " classifier = NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297af42-a5f8-45f3-8132-ab1d9ae470d5",
   "metadata": {},
   "source": [
    "## Testing: We classify new sentences using the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f23f2bc9-6e2e-45cc-b9f8-6920d27b845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I really enjoyed this movie' => Predicted Sentiment: positive\n",
      "Sentence: 'This movie was awful' => Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the classifier with new sentences\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie\",\n",
    "    \"This movie was awful\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    features = extract_features(sentence)\n",
    "    predicted_label = classifier.classify(features)\n",
    "    print(f\"Sentence: '{sentence}' => Predicted Sentiment: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0902af-56e8-483c-80d0-def38cd9948e",
   "metadata": {},
   "source": [
    " ## 1. Understanding Text Classification\n",
    " #### Text classification involves assigning a category to a given piece of text. For example:\n",
    " #### • Labeling emails as \"Spam\" or \"Not Spam.\"\n",
    " #### • Categorizing movie reviews as \"Positive\" or \"Negative.\"\n",
    " #### In this exercise, we will classify movie reviews as either Positive or Negative using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b60fbdb-6865-4495-ae77-ddb94aa20a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09957fbd-a26c-4e2c-b937-f2de50c2ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\neeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\neeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary datasets\n",
    "nltk.download('movie_reviews')  # Movie review dataset\n",
    "nltk.download('stopwords')      # Stopwords for preprocessing\n",
    "nltk.download('punkt')          # Tokenizer for text processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b254cd3-38a1-42a9-b0c5-af4525ee538a",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d284148-2a9f-4c55-bd71-56bbb9125ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Make sure you have downloaded the dataset before running this:\n",
    "# nltk.download('movie_reviews')\n",
    "\n",
    "# Create list of documents: each document is a tuple (list_of_words, category)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1259a-352c-4ede-bd53-d7e9e5ee9526",
   "metadata": {},
   "source": [
    "##  3. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6284af97-14e6-477c-9b28-d58b991c13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document (first 20 words): ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n",
      "Lowercased Words (first 20): ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n",
      "Alphabetic Words (first 20): ['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of']\n",
      "Filtered Words (No Stopwords, first 20): ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# Example of tokenizing one document for clarity\n",
    "sample_document = documents[0][0]  # Get the first document (list of words)\n",
    "print(f\"Original Document (first 20 words): {sample_document[:20]}\")\n",
    "\n",
    "# Lowercasing\n",
    "# Convert all words to lowercase\n",
    "lowercased_words = [word.lower() for word in sample_document]\n",
    "print(f\"Lowercased Words (first 20): {lowercased_words[:20]}\")\n",
    "\n",
    "# Removing Non-Alphabetic Tokens\n",
    "# Remove words that are not purely alphabetic\n",
    "alphabetic_words = [word for word in lowercased_words if word.isalpha()]\n",
    "print(f\"Alphabetic Words (first 20): {alphabetic_words[:20]}\")\n",
    "\n",
    "# Stop Word Removal\n",
    "# Load stopwords and remove them from the dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "print(f\"Filtered Words (No Stopwords, first 20): {filtered_words[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416f05f-5768-4808-be28-4a1c0d2791b6",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed2b9c5-d439-4b1e-b34e-5e72575b6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "\n",
    "# Assuming nltk data is downloaded:\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Prepare documents as (list_of_words, category) tuples\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Create a frequency distribution of all words in the corpus (lowercased and alphabetic only)\n",
    "all_words = nltk.FreqDist(\n",
    "    word.lower() for word in movie_reviews.words() if word.isalpha()\n",
    ")\n",
    "\n",
    "# Use the 2000 most common words as features\n",
    "word_features = list(all_words.keys())[:2000]\n",
    "\n",
    "# Define a feature extractor function\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in word_features}\n",
    "    return features\n",
    "\n",
    "# Load stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess all documents: lowercase, remove stopwords & non-alphabetic tokens, extract features\n",
    "preprocessed_documents = []\n",
    "for (doc, category) in documents:\n",
    "    filtered_words = [\n",
    "        word.lower() for word in doc if word.isalpha() and word.lower() not in stop_words\n",
    "    ]\n",
    "    features = document_features(filtered_words)\n",
    "    preprocessed_documents.append((features, category))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf1dcb-4382-4e1d-a875-41bbcc1445e5",
   "metadata": {},
   "source": [
    "## 5. Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d1b4299-cdb3-4313-9ca1-51b3a96c7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "train_set = preprocessed_documents[:1600]\n",
    "test_set = preprocessed_documents[1600:]\n",
    "\n",
    "# Train the Naive Bayes Classifier using the training set\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e2d39-6620-4659-b794-03726688d9a2",
   "metadata": {},
   "source": [
    "## 6. Testing and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55c3a3ad-3f59-4f09-bb6d-6c4c4d2306fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.25%\n",
      "Prediction for test review: neg\n",
      "\n",
      "Most Informative Features:\n",
      "Most Informative Features\n",
      "                   chick = True              neg : pos    =      8.6 : 1.0\n",
      "                 frances = True              pos : neg    =      8.3 : 1.0\n",
      "              undercover = True              neg : pos    =      7.8 : 1.0\n",
      "              derivative = True              neg : pos    =      7.0 : 1.0\n",
      "                  inject = True              neg : pos    =      7.0 : 1.0\n",
      "                 justify = True              neg : pos    =      6.2 : 1.0\n",
      "                   banal = True              neg : pos    =      5.8 : 1.0\n",
      "                bothered = True              neg : pos    =      5.8 : 1.0\n",
      "                     ugh = True              neg : pos    =      5.8 : 1.0\n",
      "                   waste = True              neg : pos    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Evaluate the Model\n",
    "model_accuracy = accuracy(classifier, test_set) * 100\n",
    "print(f\"Accuracy: {model_accuracy:.2f}%\")\n",
    "\n",
    "# Test with New Data\n",
    "test_review = \"This movie was absolutely great, with great performances and a good story.\"\n",
    "test_tokens = word_tokenize(test_review)\n",
    "test_words = [word.lower() for word in test_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "test_features = document_features(test_words)\n",
    "\n",
    "# Predict and display result\n",
    "prediction = classifier.classify(test_features)\n",
    "print(f\"Prediction for test review: {prediction}\")\n",
    "\n",
    "# Display the Most Informative Features\n",
    "print(\"\\nMost Informative Features:\")\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209023d-af1d-45cb-9e13-d91d048a17da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
